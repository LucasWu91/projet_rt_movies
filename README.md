# ğŸ¬ RT2025 â€” Scraper Rotten Tomatoes + Dashboard interactif

## ğŸ“Œ PrÃ©sentation du projet

Ce projet a pour objectif de construire une chaÃ®ne complÃ¨te de traitement de donnÃ©es autour des films Rotten Tomatoes 2025, en respectant les contraintes suivantes :

* Scraper un site web rÃ©el
* Stocker les donnÃ©es dans une base de donnÃ©es
* CrÃ©er une application web de visualisation
* Conteneuriser lâ€™ensemble avec Docker
* Fournir une documentation technique et fonctionnelle

Le projet simule un mini pipeline de data engineering complet, de la collecte Ã  la visualisation.

---

# ğŸ§  Architecture globale

Le systÃ¨me repose sur 3 services principaux :

1. Scraper (Scrapy)
   â†’ rÃ©cupÃ¨re les donnÃ©es sur Rotten Tomatoes

2. MongoDB
   â†’ stocke les donnÃ©es

3. Dashboard (Flask + JS)
   â†’ lit la base et affiche les rÃ©sultats

Flux de donnÃ©es :

Scrapy â†’ MongoDB â†’ Flask API â†’ Frontend (JS)

---

# âš™ï¸ Choix techniques

## ğŸ•·ï¸ Scraping : Scrapy

Choisi car :

* Plus robuste que BeautifulSoup
* Gestion native des pipelines
* Gestion des dÃ©lais (anti-ban)
* Architecture propre (spiders / pipelines)

Le scraper :

* RÃ©cupÃ¨re la liste des films 2025
* Visite chaque page
* Extrait :

  * titre
  * tomatometer
  * audience score
  * URL

---

## ğŸ—„ï¸ Base de donnÃ©es : MongoDB

Choisi car :

* AdaptÃ© aux donnÃ©es semi-structurÃ©es
* Simple Ã  connecter avec Python
* Facile Ã  utiliser en Docker
* Flexible pour Ã©voluer

Collection utilisÃ©e :

movies

---

## ğŸŒ Backend : Flask

Choisi car :

* LÃ©ger
* Rapide Ã  mettre en place
* Parfait pour exposer une API locale
* IdÃ©al pour projets acadÃ©miques

RÃ´le :

* Se connecter Ã  MongoDB
* Fournir les donnÃ©es au frontend

---

## ğŸ¨ Frontend : HTML / CSS / JavaScript

Le frontend :

* Appelle lâ€™API Flask
* Affiche :

  * Graphiques
  * Statistiques
  * Liste des films

Librairie graphique utilisÃ©e :
Chart.js

---

## ğŸ³ Conteneurisation : Docker

Docker permet :

* Dâ€™Ã©viter les conflits de versions
* De reproduire le projet facilement
* De lancer tous les services ensemble

Conteneurs :

* mongo
* scraper
* dashboard

---

# ğŸš€ Comment lancer le projet

## 1) PrÃ©requis

Installer :

* Docker Desktop
* Git

---

## 2) Lancement

Ã€ la racine du projet :

docker compose up --build

Cela dÃ©marre automatiquement :

* MongoDB
* Le scraper
* Le dashboard

---

## 3) AccÃ¨s Ã  lâ€™application

Ouvrir :

[http://127.0.0.1:8050](http://127.0.0.1:8050)

---

# ğŸ—ƒï¸ VÃ©rifier les donnÃ©es

Ouvrir MongoDB :

docker exec -it rt2025_mongo mongosh

Puis :

use rt2025
db.movies.countDocuments()

---

# ğŸ“Š FonctionnalitÃ©s du dashboard

## Page "Vue dâ€™ensemble"

* Statistiques globales

## Page "Graphiques"

* Distribution Tomatometer
* Distribution Audience Score

## Page "Films"

* Liste complÃ¨te des films scrapÃ©s

---

# âš ï¸ DifficultÃ©s rencontrÃ©es

## 1) Scraping dynamique

ProblÃ¨me :

* Rotten Tomatoes charge certaines donnÃ©es dynamiquement

Solution :

* Scraper les pages individuelles des films

---

## 2) Connexion MongoDB entre conteneurs

ProblÃ¨me :

* Le dashboard ne voyait pas les donnÃ©es

Cause :

* Mauvaise URL MongoDB

Solution :

Utiliser :

mongodb://mongo:27017

au lieu de :

localhost

---

## 3) ProblÃ¨me de port Flask

ProblÃ¨me :

* Flask tournait sur 5000
* Docker exposait 8050

Solution :

* Faire Ã©couter Flask sur 8050

---

## 4) DonnÃ©es visibles dans Mongo mais pas dans lâ€™app

ProblÃ¨me :

* Le frontend ne rÃ©cupÃ©rait rien

Cause :

* Mauvaise connexion backend â†’ Mongo

Solution :

* Corriger les variables dâ€™environnement

---

## 5) Gestion des bins graphiques

ProblÃ¨me :

* Le graphique affichait "80â€“100" deux fois

Solution :

* CrÃ©er un bin spÃ©cifique pour "100 uniquement"

---

# ğŸ“ˆ RÃ©sultat

Le projet permet maintenant :

âœ” Scraper automatiquement tous les films
âœ” Stocker 200+ films en base
âœ” Visualiser les donnÃ©es en temps rÃ©el
âœ” DÃ©ployer en un seul docker compose

---

# ğŸ§© AmÃ©liorations possibles

* Recherche par titre
* Filtres par score
* Mise Ã  jour automatique quotidienne
* Ajout dâ€™autres annÃ©es
* Export CSV

---

# ğŸ Conclusion

Ce projet met en place une chaÃ®ne complÃ¨te de traitement de donnÃ©es :

Scraping â†’ Stockage â†’ API â†’ Visualisation

Il couvre des compÃ©tences clÃ©s :

* Data scraping
* Backend Python
* Base de donnÃ©es NoSQL
* Frontend interactif
* Dockerisation

Ce type dâ€™architecture est proche de ce quâ€™on retrouve dans de vrais projets data en production.

